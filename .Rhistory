leaf <- read.csv("leaf.csv", header = FALSE,
col.names = c("Class", "Speciment_n°", "Eccentricity", "Aspect_Ratio",
"Elongation", "Solidity", "Stochastic_Convexity",
"Isoperimetric_Factor", "Maximal_Indentation_Depth",
"Lobedness", "Average_Intensity", "Average_Contrast",
"Smoothness", " Third_Moment", "Uniformity",
"Entropy"))
View(leaf)
library(dplyr)
View(leaf)
sum(complete.cases(leaf)) == nrow(leaf)
set.seed(1)
shuffled <- leaf[sample(n),]
shuffled <- leaf[sample(nrow(leaf)),]
plot(leaf)
#check if there are some NA values
sum(complete.cases(leaf)) == nrow(leaf)
K <- 10
accuracy <- rep(0,10)
View(shuffled)
?rpart
library(rpart)
?rpart
for (i in 1:K) {
# These indices indicate the interval of the test set
indices <- (((i-1) * round((1/10)*nrow(shuffled))) + 1):((i*round((1/10) * nrow(shuffled))))
#take all the rows execpt those between 1:indices
train <- shuffled[-indices,]
#take all the rows with indices = 1:indices
test <- shuffled[indices,]
tree <- rpart(Class ~ ., train, method="class")
pred <- predict(tree, test,type="class")
confusionM <- table(test$Class,pred)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
}
png(filename = "leaf_class.png")
fancyRpartPlot(tree)
dev.off()
print(paste("The accuracy for predicting the Species is: ", mean(accuracy)))
library(rpart)
library(rpart.plot)
#colored plots (use function fancyRpartPlot())
library(RColorBrewer)
library(rattle)
png(filename = "leaf_class.png")
fancyRpartPlot(tree)
dev.off()
print(paste("The accuracy for predicting the Species is: ", mean(accuracy)))
accuracy
?importance
importance(tree)
tree
for (i in 1:K) {
# These indices indicate the interval of the test set
indices <- (((i-1) * round((1/10)*nrow(shuffled))) + 1):((i*round((1/10) * nrow(shuffled))))
#take all the rows execpt those between 1:indices
train <- shuffled[-indices,]
#take all the rows with indices = 1:indices
test <- shuffled[indices,]
tree <- rpart(Class ~ ., train, method="class", minbucket=100)
pred <- predict(tree, test,type="class")
confusionM <- table(test$Class,pred)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
}
png(filename = "leaf_class.png")
fancyRpartPlot(tree)
dev.off()
print(paste("The accuracy for predicting the Species is: ", mean(accuracy)))
for (i in 1:K) {
# These indices indicate the interval of the test set
indices <- (((i-1) * round((1/10)*nrow(shuffled))) + 1):((i*round((1/10) * nrow(shuffled))))
#take all the rows execpt those between 1:indices
train <- shuffled[-indices,]
#take all the rows with indices = 1:indices
test <- shuffled[indices,]
tree <- rpart(Class ~ ., train, method="class", minbucket=10)
pred <- predict(tree, test,type="class")
confusionM <- table(test$Class,pred)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
}
png(filename = "leaf_class.png")
fancyRpartPlot(tree)
dev.off()
print(paste("The accuracy for predicting the Species is: ", mean(accuracy)))
for (i in 1:K) {
# These indices indicate the interval of the test set
indices <- (((i-1) * round((1/20)*nrow(shuffled))) + 1):((i*round((1/20) * nrow(shuffled))))
#take all the rows execpt those between 1:indices
train <- shuffled[-indices,]
#take all the rows with indices = 1:indices
test <- shuffled[indices,]
tree <- rpart(Class ~ ., train, method="class", minbucket=10)
pred <- predict(tree, test,type="class")
confusionM <- table(test$Class,pred)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
}
png(filename = "leaf_class.png")
fancyRpartPlot(tree)
dev.off()
print(paste("The accuracy for predicting the Species is: ", mean(accuracy)))
print(K)
library(randomForest)
nTrees <- c(1,10,25,50,75,100,250,500,750,1000,1250,1500,1750,2000,2250,2500,2750,3000)
for(numbOfTrees in nTrees) {
accuracy <- rep(0,10)
#Cross Validation
for (i in 1:K) {
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
#train
#rf <- randomForest(Species ~ ., train)
#less influent vars
rf <- randomForest(Class ~ ., train)
#most influent vars
#rf <- randomForest(Species ~ Petal.Length + Petal.Width, train, ntree = numbOfTrees)
#prediction
predicted <- predict(rf, test)
confusionM <- table(test$Class, predicted)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
errRateTemp <- rf[["err.rate"]]
errRates[i] <- errRateTemp[numbOfTrees][1]
}
print(paste("The accuracy of RF for predicting species with ntree=",
numbOfTrees," is :", mean(accuracy)))
print(paste("The OOB train error rate is: ", mean(errRates)))
}
n <- nrows(leaf)
for(numbOfTrees in nTrees) {
accuracy <- rep(0,10)
#Cross Validation
for (i in 1:K) {
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
#train
#rf <- randomForest(Species ~ ., train)
#less influent vars
rf <- randomForest(Class ~ ., train)
#most influent vars
#rf <- randomForest(Species ~ Petal.Length + Petal.Width, train, ntree = numbOfTrees)
#prediction
predicted <- predict(rf, test)
confusionM <- table(test$Class, predicted)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
errRateTemp <- rf[["err.rate"]]
errRates[i] <- errRateTemp[numbOfTrees][1]
}
print(paste("The accuracy of RF for predicting species with ntree=",
numbOfTrees," is :", mean(accuracy)))
print(paste("The OOB train error rate is: ", mean(errRates)))
}
n <- nrows(leaf)
n <- nrow(leaf)
for(numbOfTrees in nTrees) {
accuracy <- rep(0,10)
#Cross Validation
for (i in 1:K) {
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
#train
#rf <- randomForest(Species ~ ., train)
#less influent vars
rf <- randomForest(Class ~ ., train)
#most influent vars
#rf <- randomForest(Species ~ Petal.Length + Petal.Width, train, ntree = numbOfTrees)
#prediction
predicted <- predict(rf, test)
confusionM <- table(test$Class, predicted)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
errRateTemp <- rf[["err.rate"]]
errRates[i] <- errRateTemp[numbOfTrees][1]
}
print(paste("The accuracy of RF for predicting species with ntree=",
numbOfTrees," is :", mean(accuracy)))
print(paste("The OOB train error rate is: ", mean(errRates)))
}
errRates <- c()
for(numbOfTrees in nTrees) {
accuracy <- rep(0,10)
#Cross Validation
for (i in 1:K) {
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
#train
#rf <- randomForest(Species ~ ., train)
#less influent vars
rf <- randomForest(Class ~ ., train)
#most influent vars
#rf <- randomForest(Species ~ Petal.Length + Petal.Width, train, ntree = numbOfTrees)
#prediction
predicted <- predict(rf, test)
confusionM <- table(test$Class, predicted)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
errRateTemp <- rf[["err.rate"]]
errRates[i] <- errRateTemp[numbOfTrees][1]
}
print(paste("The accuracy of RF for predicting species with ntree=",
numbOfTrees," is :", mean(accuracy)))
print(paste("The OOB train error rate is: ", mean(errRates)))
}
rf <- randomForest(Class ~ ., data = shuffled)
predictForest <- predict(rf, shuffled)
confusionM <- table(shuffled$Class, predictForest)
print(paste("Accuracy: ", sum(diag(confusionM))/sum(confusionM)))
importance(rf)
importance?
?importance
?importance
for(numbOfTrees in nTrees) {
accuracy <- rep(0,10)
#Cross Validation
for (i in 1:K) {
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
#train
#rf <- randomForest(Species ~ ., train)
#less influent vars
rf <- randomForest(as.factor(Class) ~ ., train)
#most influent vars
#rf <- randomForest(Species ~ Petal.Length + Petal.Width, train, ntree = numbOfTrees)
#prediction
predicted <- predict(rf, test)
confusionM <- table(test$Class, predicted)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
errRateTemp <- rf[["err.rate"]]
errRates[i] <- errRateTemp[numbOfTrees][1]
}
print(paste("The accuracy of RF for predicting species with ntree=",
numbOfTrees," is :", mean(accuracy)))
print(paste("The OOB train error rate is: ", mean(errRates)))
}
library(dplyr)
library(rpart)
library(rpart.plot)
#colored plots (use function fancyRpartPlot())
library(RColorBrewer)
library(rattle)
leaf <- read.csv("leaf.csv", header = FALSE,
col.names = c("Class", "Speciment_n°", "Eccentricity", "Aspect_Ratio",
"Elongation", "Solidity", "Stochastic_Convexity",
"Isoperimetric_Factor", "Maximal_Indentation_Depth",
"Lobedness", "Average_Intensity", "Average_Contrast",
"Smoothness", " Third_Moment", "Uniformity",
"Entropy"))
leaf$Class <- as.factor(leaf$Class)
set.seed(1)
K <- 10
#check if there are some NA values
sum(complete.cases(leaf)) == nrow(leaf)
shuffled <- leaf[sample(nrow(leaf)),]
accuracy <- rep(0,10)
for (i in 1:K) {
# These indices indicate the interval of the test set
indices <- (((i-1) * round((1/20)*nrow(shuffled))) + 1):((i*round((1/20) * nrow(shuffled))))
#take all the rows execpt those between 1:indices
train <- shuffled[-indices,]
#take all the rows with indices = 1:indices
test <- shuffled[indices,]
tree <- rpart(Class ~ ., train, method="class", minbucket=10)
pred <- predict(tree, test,type="class")
confusionM <- table(test$Class,pred)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
}
png(filename = "leaf_class.png")
fancyRpartPlot(tree)
dev.off()
print(paste("The accuracy for predicting the Species is: ", mean(accuracy)))
library(randomForest)
n <- nrow(leaf)
nTrees <- c(1,10,25,50,75,100,250,500,750,1000,1250,1500,1750,2000,2250,2500,2750,3000)
errRates <- c()
for(numbOfTrees in nTrees) {
accuracy <- rep(0,10)
#Cross Validation
for (i in 1:K) {
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
#train
#rf <- randomForest(Species ~ ., train)
#less influent vars
rf <- randomForest(as.factor(Class) ~ ., train)
#most influent vars
#rf <- randomForest(Species ~ Petal.Length + Petal.Width, train, ntree = numbOfTrees)
#prediction
predicted <- predict(rf, test)
confusionM <- table(test$Class, predicted)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
errRateTemp <- rf[["err.rate"]]
errRates[i] <- errRateTemp[numbOfTrees][1]
}
print(paste("The accuracy of RF for predicting species with ntree=",
numbOfTrees," is :", mean(accuracy)))
print(paste("The OOB train error rate is: ", mean(errRates)))
}
# 1. investigate variable importance with RF
rf <- randomForest(as.factor(Class) ~ ., data = shuffled)
predictForest <- predict(rf, shuffled)
confusionM <- table(shuffled$Class, predictForest)
print(paste("Accuracy: ", sum(diag(confusionM))/sum(confusionM)))
importance(rf)
View(rf)
?fancyRpartPlot
?png
png(filename = "leaf_class.png", wirdth=1000, height = 1000)
fancyRpartPlot(tree)
dev.off()
print(paste("The accuracy for predicting the Species is: ", mean(accuracy)))
for (i in 1:K) {
# These indices indicate the interval of the test set
indices <- (((i-1) * round((1/20)*nrow(shuffled))) + 1):((i*round((1/20) * nrow(shuffled))))
#take all the rows execpt those between 1:indices
train <- shuffled[-indices,]
#take all the rows with indices = 1:indices
test <- shuffled[indices,]
tree <- rpart(Class ~ ., train, method="class", minbucket=10)
pred <- predict(tree, test,type="class")
confusionM <- table(test$Class,pred)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
}
png(filename = "leaf_class.png", wirdth=1000, height = 1000)
fancyRpartPlot(tree)
dev.off()
print(paste("The accuracy for predicting the Species is: ", mean(accuracy)))
print(paste("The accuracy for predicting the Species is: ", mean(accuracy)))
shuffled <- leaf[sample(n),]
for(numbOfTrees in nTrees) {
accuracy <- rep(0,10)
#Cross Validation
for (i in 1:K) {
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
#train
#rf <- randomForest(Species ~ ., train)
#less influent vars
rf <- randomForest(as.factor(Class) ~ ., train)
#most influent vars
#rf <- randomForest(Species ~ Petal.Length + Petal.Width, train, ntree = numbOfTrees)
#prediction
predicted <- predict(rf, test)
confusionM <- table(test$Class, predicted)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
errRateTemp <- rf[["err.rate"]]
errRates[i] <- errRateTemp[numbOfTrees][1]
}
print(paste("The accuracy of RF for predicting species with ntree=",
numbOfTrees," is :", mean(accuracy)))
print(paste("The OOB train error rate is: ", mean(errRates)))
}
rf <- randomForest(as.factor(Class) ~ ., data = shuffled)
predictForest <- predict(rf, shuffled)
confusionM <- table(shuffled$Class, predictForest)
print(paste("Accuracy: ", sum(diag(confusionM))/sum(confusionM)))
importance(rf)
# 1. investigate variable importance with RF
rf <- randomForest(as.factor(Class) ~ ., data = shuffled, importance = TRUE)
predictForest <- predict(rf, shuffled)
confusionM <- table(shuffled$Class, predictForest)
print(paste("Accuracy: ", sum(diag(confusionM))/sum(confusionM)))
rf <- randomForest(as.factor(Class) ~ ., data = shuffled, importance = TRUE)
predictForest <- predict(rf, shuffled)
confusionM <- table(shuffled$Class, predictForest)
print(paste("Accuracy: ", sum(diag(confusionM))/sum(confusionM)))
importance(rf)
importance(rf, scale = FALSE)
rf <- randomForest(as.factor(Class) ~ ., data = shuffled, importance = TRUE)
predictForest <- predict(rf, shuffled)
confusionM <- table(shuffled$Class, predictForest)
print(paste("Accuracy: ", sum(diag(confusionM))/sum(confusionM)))
importance(rf, scale = FALSE)
View(shuffled)
varImpPlot(rf)
importance(rf, scale = TRUE)
importance(rf, scale = TRUE)
varImpPlot(rf)
RF
rf
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
i <- 1
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
i <- 2
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
i <- 3
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
View(train)
View(test)
View(test)
View(train)
View(test)
rm(test)
rm(train)
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
View(test)
i <- 2
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
View(test)
library(randomForest)
n <- nrow(leaf)
nTrees <- c(1,10,25,50,75,100,250,500,750,1000,1250,1500,1750,2000,2250,2500,2750,3000)
errRates <- c()
shuffled <- leaf[sample(n),]
#i <- 2
#indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
#train <- shuffled[-indexes,]
#test <- shuffled[indexes,]
for(numbOfTrees in nTrees) {
accuracy <- rep(0,10)
#Cross Validation
for (i in 1:K) {
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
#train
#rf <- randomForest(Species ~ ., train)
#less influent vars
rf <- randomForest(as.factor(Class) ~ ., train)
#most influent vars
#rf <- randomForest(Species ~ Petal.Length + Petal.Width, train, ntree = numbOfTrees)
#prediction
predicted <- predict(rf, test)
confusionM <- table(test$Class, predicted)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
errRateTemp <- rf[["err.rate"]]
errRates[i] <- errRateTemp[numbOfTrees][1]
}
print(paste("The accuracy of RF for predicting species with ntree=",
numbOfTrees," is :", mean(accuracy)))
print(paste("The OOB train error rate is: ", mean(errRates)))
}
#i <- 2
#indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
#train <- shuffled[-indexes,]
#test <- shuffled[indexes,]
for(numbOfTrees in nTrees) {
accuracy <- rep(0,10)
#Cross Validation
for (i in 1:K) {
indexes <- ((i-1)*round(1/10*n) + 1):i*round(1/10*n)
train <- shuffled[-indexes,]
test <- shuffled[indexes,]
#train
#rf <- randomForest(Species ~ ., train)
#less influent vars
rf <- randomForest(as.factor(Class) ~ ., train, ntree = numbOfTrees)
#most influent vars
#rf <- randomForest(Species ~ Petal.Length + Petal.Width, train, ntree = numbOfTrees)
#prediction
predicted <- predict(rf, test)
confusionM <- table(test$Class, predicted)
accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
errRateTemp <- rf[["err.rate"]]
errRates[i] <- errRateTemp[numbOfTrees][1]
}
print(paste("The accuracy of RF for predicting species with ntree=",
numbOfTrees," is :", mean(accuracy)))
print(paste("The OOB train error rate is: ", mean(errRates)))
}
